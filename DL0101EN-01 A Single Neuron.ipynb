{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.005182,
     "end_time": "2020-10-20T20:57:35.483436",
     "exception": false,
     "start_time": "2020-10-20T20:57:35.478254",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Welcome to Deep Learning! #\n",
    "\n",
    "Welcome to **Introduction to Deep Learning** course! You're about to learn all you need to get started building your own deep neural networks. Using Keras(NN Modeling) and Tensorflow(Numpy) you'll learn how to:\n",
    "- Create a **fully-connected** neural network architecture\n",
    "- Apply neural nets to two classic ML problems: **regression** and **classification**\n",
    "- Train neural nets with **stochastic gradient descent**, and\n",
    "- Improve performance with **dropout**, **batch normalization**, and other techniques\n",
    "\n",
    "The tutorials will introduce you to these topics with fully-worked examples, and then in the exercises, you'll explore these topics in more depth and apply them to real-world datasets.\n",
    "\n",
    "Let's get started!\n",
    "\n",
    "# What is Deep Learning? #\n",
    "\n",
    "Some of the most impressive advances in artificial intelligence in recent years have been in the field of **deep learning**. `Natural language translation, image recognition, and game playing` are all tasks where deep learning models have neared or even exceeded human-level performance.\n",
    "\n",
    "So what is deep learning? **Deep learning** is an approach to machine learning characterized by deep stacks of computations. This depth of computation is what has enabled deep learning models to disentangle the kinds of complex and hierarchical patterns found in the most challenging real-world datasets.\n",
    "\n",
    "Through their power and scalability **neural networks** have become the defining model of deep learning.  Neural networks are composed of neurons, where each neuron individually performs only a simple computation. The power of a neural network comes instead from the complexity of the connections these neurons can form.\n",
    "\n",
    "# The Linear Unit #\n",
    "\n",
    "So let's begin with the fundamental component of a neural network: the individual neuron. As a diagram, a **neuron** (or **unit**) with one input looks like:\n",
    "\n",
    "<figure style=\"padding: 1em;\">\n",
    "<img src=\"https://i.imgur.com/mfOlDR6.png\" width=\"250\" alt=\"Diagram of a linear unit.\"/>\n",
    "    <figcaption style=\"textalign: center; font-style: italic\"><center>The Linear Unit: $y = w x + b$\n",
    "</center></figcaption>\n",
    "</figure>\n",
    "\n",
    "The input is `x`. Its connection to the neuron has a **weight** which is `w`. Whenever a value flows through a connection, you multiply the value by the connection's weight. For the input `x`, what reaches the neuron is `w * x`. A neural network \"learns\" by modifying its weights.\n",
    "\n",
    "The `b` is a special kind of weight we call the **bias**. The bias doesn't have any input data associated with it; instead, we put a `1` in the diagram so that the value that reaches the neuron is just `b` (since `1 * b = b`). The bias enables the neuron to modify the output independently of its inputs.\n",
    "\n",
    "The `y` is the value the neuron ultimately outputs. To get the output, the neuron sums up all the values it receives through its connections. This neuron's activation is `y = w * x + b`, or as a formula $y = w x + b$.\n",
    "\n",
    "<blockquote style=\"margin-right:auto; margin-left:auto; background-color: #ebf9ff; padding: 1em; margin:24px;\">\n",
    "    <strong>Does the formula $y=w x + b$ look familiar?</strong><br>\n",
    "It's an equation of a line! It's the slope-intercept equation, where $w$ is the slope and $b$ is the y-intercept. \n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Providing the Data\n",
    "\n",
    "Next up we'll feed in some data. In this case we are taking 6 xs and 6ys. You can see that the relationship between these is that y=2x-1, so where x = -1, y=-3 etc. etc. \n",
    "\n",
    "* `2`: Slope or Weight\n",
    "* `-1`: bias or Intreception \n",
    "\n",
    "\n",
    "A python library called 'Numpy' provides lots of array type data structures that are a defacto standard way of doing it. We declare that we want to use these by specifying the values as an np.array[]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = np.array([-1.0,  0.0, 1.0, 2.0, 3.0, 4.0], dtype=float)\n",
    "ys = np.array([-3.0, -1.0, 1.0, 3.0, 5.0, 7.0], dtype=float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|X|Y|\n",
    "|--|--|\n",
    "|4.0|7.0|\n",
    "|5.0|9.0|\n",
    "|6.0|11.0|\n",
    "|7.0|13.0|\n",
    "|8.0|15.0|\n",
    "|9.0|17.0|\n",
    "|10.0|?|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6,)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs.ndim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* if X is Varing 1 time then Y will Varing from 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y = mx+b\n",
    "slope = 2\n",
    "bias = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3., -1.,  1.,  3.,  5.,  7.])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = slope*xs+bias\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19.0"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_x_values = 10.0\n",
    "y = slope*new_x_values+bias\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.99, -1.  , -0.01,  0.98,  1.97,  2.96])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slope1 = 0.99\n",
    "y = slope1*xs+bias\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow  as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([keras.layers.Dense(units=1,input_shape=[1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.engine.sequential.Sequential at 0x2b887382d30>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='sgd',loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/125\n",
      "1/1 [==============================] - 0s 994us/step - loss: 0.0194\n",
      "Epoch 2/125\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.0190\n",
      "Epoch 3/125\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.0186\n",
      "Epoch 4/125\n",
      "1/1 [==============================] - 0s 986us/step - loss: 0.0183\n",
      "Epoch 5/125\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.0179\n",
      "Epoch 6/125\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.0175\n",
      "Epoch 7/125\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.0171\n",
      "Epoch 8/125\n",
      "1/1 [==============================] - 0s 997us/step - loss: 0.0168\n",
      "Epoch 9/125\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.0165\n",
      "Epoch 10/125\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.0161\n",
      "Epoch 11/125\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.0158\n",
      "Epoch 12/125\n",
      "1/1 [==============================] - 0s 997us/step - loss: 0.0155\n",
      "Epoch 13/125\n",
      "1/1 [==============================] - 0s 997us/step - loss: 0.0151\n",
      "Epoch 14/125\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.0148\n",
      "Epoch 15/125\n",
      "1/1 [==============================] - 0s 997us/step - loss: 0.0145\n",
      "Epoch 16/125\n",
      "1/1 [==============================] - 0s 997us/step - loss: 0.0142\n",
      "Epoch 17/125\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.0139\n",
      "Epoch 18/125\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0136\n",
      "Epoch 19/125\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0134\n",
      "Epoch 20/125\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.0131\n",
      "Epoch 21/125\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.0128\n",
      "Epoch 22/125\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.0126\n",
      "Epoch 23/125\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0123\n",
      "Epoch 24/125\n",
      "1/1 [==============================] - 0s 997us/step - loss: 0.0121\n",
      "Epoch 25/125\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.0118\n",
      "Epoch 26/125\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.0116\n",
      "Epoch 27/125\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.0113\n",
      "Epoch 28/125\n",
      "1/1 [==============================] - 0s 997us/step - loss: 0.0111\n",
      "Epoch 29/125\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.0109\n",
      "Epoch 30/125\n",
      "1/1 [==============================] - 0s 997us/step - loss: 0.0106\n",
      "Epoch 31/125\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.0104\n",
      "Epoch 32/125\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0102\n",
      "Epoch 33/125\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.0100\n",
      "Epoch 34/125\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.0098\n",
      "Epoch 35/125\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.0096\n",
      "Epoch 36/125\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.0094\n",
      "Epoch 37/125\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.0092\n",
      "Epoch 38/125\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.0090\n",
      "Epoch 39/125\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0088\n",
      "Epoch 40/125\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.0086\n",
      "Epoch 41/125\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.0085\n",
      "Epoch 42/125\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.0083\n",
      "Epoch 43/125\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0081\n",
      "Epoch 44/125\n",
      "1/1 [==============================] - 0s 997us/step - loss: 0.0080\n",
      "Epoch 45/125\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.0078\n",
      "Epoch 46/125\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.0076\n",
      "Epoch 47/125\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.0075\n",
      "Epoch 48/125\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.0073\n",
      "Epoch 49/125\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.0072\n",
      "Epoch 50/125\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0070\n",
      "Epoch 51/125\n",
      "1/1 [==============================] - 0s 997us/step - loss: 0.0069\n",
      "Epoch 52/125\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.0067\n",
      "Epoch 53/125\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0066\n",
      "Epoch 54/125\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.0065\n",
      "Epoch 55/125\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.0063\n",
      "Epoch 56/125\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.0062\n",
      "Epoch 57/125\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0061\n",
      "Epoch 58/125\n",
      "1/1 [==============================] - 0s 991us/step - loss: 0.0060\n",
      "Epoch 59/125\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0058\n",
      "Epoch 60/125\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.0057\n",
      "Epoch 61/125\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.0056\n",
      "Epoch 62/125\n",
      "1/1 [==============================] - 0s 997us/step - loss: 0.0055\n",
      "Epoch 63/125\n",
      "1/1 [==============================] - 0s 996us/step - loss: 0.0054\n",
      "Epoch 64/125\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.0053\n",
      "Epoch 65/125\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.0051\n",
      "Epoch 66/125\n",
      "1/1 [==============================] - 0s 997us/step - loss: 0.0050\n",
      "Epoch 67/125\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.0049\n",
      "Epoch 68/125\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.0048\n",
      "Epoch 69/125\n",
      "1/1 [==============================] - 0s 997us/step - loss: 0.0047\n",
      "Epoch 70/125\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.0046\n",
      "Epoch 71/125\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.0045\n",
      "Epoch 72/125\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.0044\n",
      "Epoch 73/125\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0044\n",
      "Epoch 74/125\n",
      "1/1 [==============================] - 0s 997us/step - loss: 0.0043\n",
      "Epoch 75/125\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.0042\n",
      "Epoch 76/125\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.0041\n",
      "Epoch 77/125\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.0040\n",
      "Epoch 78/125\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.0039\n",
      "Epoch 79/125\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.0038\n",
      "Epoch 80/125\n",
      "1/1 [==============================] - 0s 997us/step - loss: 0.0038\n",
      "Epoch 81/125\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0037\n",
      "Epoch 82/125\n",
      "1/1 [==============================] - 0s 997us/step - loss: 0.0036\n",
      "Epoch 83/125\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.0035\n",
      "Epoch 84/125\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.0035\n",
      "Epoch 85/125\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0034\n",
      "Epoch 86/125\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.0033\n",
      "Epoch 87/125\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.0033\n",
      "Epoch 88/125\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.0032\n",
      "Epoch 89/125\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.0031\n",
      "Epoch 90/125\n",
      "1/1 [==============================] - 0s 997us/step - loss: 0.0031\n",
      "Epoch 91/125\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.0030\n",
      "Epoch 92/125\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0029\n",
      "Epoch 93/125\n",
      "1/1 [==============================] - 0s 996us/step - loss: 0.0029\n",
      "Epoch 94/125\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.0028\n",
      "Epoch 95/125\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.0028\n",
      "Epoch 96/125\n",
      "1/1 [==============================] - 0s 997us/step - loss: 0.0027\n",
      "Epoch 97/125\n",
      "1/1 [==============================] - 0s 997us/step - loss: 0.0026\n",
      "Epoch 98/125\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.0026\n",
      "Epoch 99/125\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.0025\n",
      "Epoch 100/125\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.0025\n",
      "Epoch 101/125\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0024\n",
      "Epoch 102/125\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.0024\n",
      "Epoch 103/125\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.0023\n",
      "Epoch 104/125\n",
      "1/1 [==============================] - 0s 996us/step - loss: 0.0023\n",
      "Epoch 105/125\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.0022\n",
      "Epoch 106/125\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0022\n",
      "Epoch 107/125\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.0022\n",
      "Epoch 108/125\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.0021\n",
      "Epoch 109/125\n",
      "1/1 [==============================] - 0s 997us/step - loss: 0.0021\n",
      "Epoch 110/125\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.0020\n",
      "Epoch 111/125\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0020\n",
      "Epoch 112/125\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.0019\n",
      "Epoch 113/125\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.0019\n",
      "Epoch 114/125\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0019\n",
      "Epoch 115/125\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0018\n",
      "Epoch 116/125\n",
      "1/1 [==============================] - 0s 996us/step - loss: 0.0018\n",
      "Epoch 117/125\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0017\n",
      "Epoch 118/125\n",
      "1/1 [==============================] - 0s 997us/step - loss: 0.0017\n",
      "Epoch 119/125\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.0017\n",
      "Epoch 120/125\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.0016\n",
      "Epoch 121/125\n",
      "1/1 [==============================] - 0s 997us/step - loss: 0.0016\n",
      "Epoch 122/125\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.0016\n",
      "Epoch 123/125\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.0015\n",
      "Epoch 124/125\n",
      "1/1 [==============================] - 0s 997us/step - loss: 0.0015\n",
      "Epoch 125/125\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.0015\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2b887410dd8>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(xs,ys,epochs=125)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[18.88771]], dtype=float32)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict([10.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.003066,
     "end_time": "2020-10-20T20:57:35.490069",
     "exception": false,
     "start_time": "2020-10-20T20:57:35.487003",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Example - The Linear Unit as a Model #\n",
    "\n",
    "Though individual neurons will usually only function as part of a larger network, it's often useful to start with a single neuron model as a baseline. Single neuron models are *linear* models. \n",
    "\n",
    "Let's think about how this might work on a dataset like [80 Cereals](https://www.kaggle.com/crawford/80-cereals). Training a model with `'sugars'` (grams of sugars per serving) as input and `'calories'` (calories per serving) as output, we might find the bias is `b=90` and the weight is `w=2.5`. We could estimate the calorie content of a cereal with 5 grams of sugar per serving like this:\n",
    "\n",
    "<figure style=\"padding: 1em;\">\n",
    "<img src=\"https://i.imgur.com/yjsfFvY.png\" width=\"1000\" alt=\"Computing with the linear unit.\">\n",
    "<figcaption style=\"textalign: center; font-style: italic\"><center>Computing with the linear unit.\n",
    "</center></figcaption>\n",
    "</figure>\n",
    "\n",
    "And, checking against our formula, we have $calories = 2.5 \\times 5 + 90 = 102.5$, just like we expect.\n",
    "\n",
    "# Multiple Inputs #\n",
    "\n",
    "The *80 Cereals* dataset has many more features than just `'sugars'`. What if we wanted to expand our model to include things like fiber or protein content? That's easy enough. We can just add more input connections to the neuron, one for each additional feature. To find the output, we would multiply each input to its connection weight and then add them all together.\n",
    "\n",
    "<figure style=\"padding: 1em;\">\n",
    "<img src=\"https://i.imgur.com/vyXSnlZ.png\" width=\"300\" alt=\"Three input connections: x0, x1, and x2, along with the bias.\">\n",
    "<figcaption style=\"textalign: center; font-style: italic\"><center>A linear unit with three inputs.\n",
    "</center></figcaption>\n",
    "</figure>\n",
    "\n",
    "The formula for this neuron would be $y = w_0 x_0 + w_1 x_1 + w_2 x_2 + b$. A linear unit with two inputs will fit a plane, and a unit with more inputs than that will fit a hyperplane.\n",
    "\n",
    "# Linear Units in Keras #\n",
    "\n",
    "The easiest way to create a model in Keras is through `keras.Sequential`, which creates a neural network as a stack of *layers*. We can create models like those above using a *dense* layer (which we'll learn more about in the next lesson).\n",
    "\n",
    "We could define a linear model accepting three input features (`'sugars'`, `'fiber'`, and `'protein'`) and producing a single output (`'calories'`) like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "papermill": {
     "duration": 5.141491,
     "end_time": "2020-10-20T20:57:40.634886",
     "exception": false,
     "start_time": "2020-10-20T20:57:35.493395",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Create a network with 1 linear unit\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(units=1, input_shape=[3])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.00331,
     "end_time": "2020-10-20T20:57:40.642192",
     "exception": false,
     "start_time": "2020-10-20T20:57:40.638882",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "With the first argument, `units`, we define how many outputs we want. In this case we are just predicting `'calories'`, so we'll use `units=1`. \n",
    "\n",
    "With the second argument, `input_shape`, we tell Keras the dimensions of the inputs. Setting `input_shape=[3]` ensures the model will accept three features as input (`'sugars'`, `'fiber'`, and `'protein'`).\n",
    "\n",
    "This model is now ready to be fit to training data!\n",
    "\n",
    "<blockquote style=\"margin-right:auto; margin-left:auto; background-color: #ebf9ff; padding: 1em; margin:24px;\">\n",
    "    <strong>Why is <code>input_shape</code> a Python list?</strong><br>\n",
    "The data we'll use in this course will be tabular data, like in a Pandas dataframe. We'll have one input for each feature in the dataset. The features are arranged by column, so we'll always have <code>input_shape=[num_columns]</code>.\n",
    "\n",
    "The reason Keras uses a list here is to permit use of more complex datasets. Image data, for instance, might need three dimensions: <code>[height, width, channels]</code>.\n",
    "</blockquote>\n",
    "\n",
    "# Your Turn #\n",
    "\n",
    "[**Define a linear model**](https://www.kaggle.com/kernels/fork/11887334) for the *Red Wine Quality* dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.003083,
     "end_time": "2020-10-20T20:57:40.648822",
     "exception": false,
     "start_time": "2020-10-20T20:57:40.645739",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "*Have questions or comments? Visit the [Learn Discussion forum](https://www.kaggle.com/learn-forum/191966) to chat with other Learners.*"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "nteract": {
   "version": "0.26.0"
  },
  "papermill": {
   "duration": 9.021854,
   "end_time": "2020-10-20T20:57:40.759246",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-10-20T20:57:31.737392",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
