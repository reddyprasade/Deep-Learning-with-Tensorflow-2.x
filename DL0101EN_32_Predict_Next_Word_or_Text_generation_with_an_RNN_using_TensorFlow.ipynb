{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "DL0101EN-32-Predict Next Word or Text generation with an RNN using TensorFlow.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZecB2FRc-JS"
      },
      "source": [
        "import os\r\n",
        "import time\r\n",
        "import tensorflow as tf\r\n",
        "from tensorflow.keras.layers.experimental import preprocessing"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1eyFTdvdHxR"
      },
      "source": [
        "### Data Collection:\r\n",
        "\r\n",
        "* [Shakespeare dataset](https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt)\r\n",
        "* [Shakespeare dataset](https://raw.githubusercontent.com/reddyprasade/Deep-Learning-with-Tensorflow-2.x/main/Data/input.txt)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZ2HoEq-dG-k"
      },
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt','https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GjBuiUOndvQB",
        "outputId": "5c22a12e-4190-4f79-d6d8-c8bb8bff59f8"
      },
      "source": [
        "# Read the text file data\r\n",
        "text = open(path_to_file,'rb').read().decode(encoding='utf-8')\r\n",
        "print(\"Length of text:{} charaters\".format(len(text)))"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of text:1115394 charaters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oSyq9LszeTvy",
        "outputId": "af8ce1da-aaca-4f4e-a2fb-837967ec0d1d"
      },
      "source": [
        "print(text[:250])"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bSLObnuKeoRI",
        "outputId": "8a2a722d-8ec8-446c-fde4-bc8acb47c17b"
      },
      "source": [
        "vocab = sorted(set(text))\r\n",
        "print(\"{} Unique Characters From Text Data\".format(len(vocab)))"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "65 Unique Characters From Text Data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ljkrmmJqfNoZ"
      },
      "source": [
        "example_text = ['abcdefg','xyz']"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kIKwOsM4fl4h",
        "outputId": "231d9009-34d6-4639-bfa5-0570b8c03f6d"
      },
      "source": [
        "chars = tf.strings.unicode_split(example_text,input_encoding='UTF-8')\r\n",
        "chars"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLljHxHffzr0"
      },
      "source": [
        "ids_from_chars = preprocessing.StringLookup(vocabulary=list(vocab))"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-WfODi8ogLUY",
        "outputId": "60767365-7bce-4375-f107-1d34d500056e"
      },
      "source": [
        "ids_from_chars"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.layers.preprocessing.string_lookup.StringLookup at 0x7f818a9bdac8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h24jYa-4gMgZ",
        "outputId": "2bf4e9ee-a93f-451d-ee4b-923a4f3dc524"
      },
      "source": [
        "ids = ids_from_chars(chars)\r\n",
        "ids"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[41, 42, 43, 44, 45, 46, 47], [64, 65, 66]]>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mR_ZXc3sgT6x"
      },
      "source": [
        "chars_from_ids = tf.keras.layers.experimental.preprocessing.StringLookup(\r\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True)"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CZoRi_QVgaF7",
        "outputId": "f6877e8e-08b3-412f-f85e-b099fe485b45"
      },
      "source": [
        "chars = chars_from_ids(ids)\r\n",
        "chars"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ngspfT7gfzM",
        "outputId": "391384a3-4691-45cb-b162-c559120829b9"
      },
      "source": [
        "tf.strings.reduce_join(chars,axis=-1).numpy()"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([b'abcdefg', b'xyz'], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDLB-ZKPgsaO"
      },
      "source": [
        "def text_from_ids(ids):\r\n",
        "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YavftYvXgz3v",
        "outputId": "632ae3b9-7cae-4221-bab8-a45ae8c22c69"
      },
      "source": [
        "all_ids = ids_from_chars(tf.strings.unicode_split(text,'UTF-8'))\r\n",
        "all_ids"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([20, 49, 58, ..., 47, 10,  2])>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uaNEduXvhDjM"
      },
      "source": [
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gTKFyYdNhMM2",
        "outputId": "e75e71bc-c5da-4f88-d75d-e78f6d9c1ffe"
      },
      "source": [
        "for ids in ids_dataset.take(10):\r\n",
        "  print(chars_from_ids(ids).numpy().decode('utf-8'))"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n",
            " \n",
            "C\n",
            "i\n",
            "t\n",
            "i\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "COk5N5LGlHAD",
        "outputId": "eabf3af3-f1c9-41f1-b728-ea1c1274537d"
      },
      "source": [
        "len(text)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1115394"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zN8x_mUhauq"
      },
      "source": [
        "seq_length = 100\r\n",
        "example_per_epoch =len(text)//(seq_length+1)"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "znJ6tH9-hzHl",
        "outputId": "90996aee-10af-466d-b0d3-62110aa686f9"
      },
      "source": [
        "sequences = ids_dataset.batch(seq_length+1,drop_remainder=True)\r\n",
        "\r\n",
        "for seq in sequences.take(1):\r\n",
        "  print(chars_from_ids(seq))"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
            " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
            " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
            " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
            " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
            " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
            " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
            " b'o' b'u' b' '], shape=(101,), dtype=string)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0OJOGuDaiNwI",
        "outputId": "58584d08-68b5-44c8-a4f2-72c4dfd29382"
      },
      "source": [
        "for seq in sequences.take(5):\r\n",
        "  print(text_from_ids(seq).numpy())"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
            "b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
            "b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
            "b'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y261JLveicAQ"
      },
      "source": [
        "def split_input_target(sequences):\r\n",
        "  input_text = sequences[:-1]\r\n",
        "  target_text = sequences[1:]\r\n",
        "  return input_text,target_text\r\n"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hrJjcWTPjTYo",
        "outputId": "15f679c1-2ad4-4035-f362-aa3e59440d0f"
      },
      "source": [
        "split_input_target(list('Python'))"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['P', 'y', 't', 'h', 'o'], ['y', 't', 'h', 'o', 'n'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Azpci1u-jZzB"
      },
      "source": [
        "dataset = sequences.map(split_input_target)"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xmKYZTyBj7eF",
        "outputId": "5246d220-84b1-438e-b9dd-481ad6dcf293"
      },
      "source": [
        "for input_example, target_example in dataset.take(1):\r\n",
        "  print(\"Input :\", text_from_ids(input_example).numpy())\r\n",
        "  print(\"Target:\", text_from_ids(target_example).numpy())"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input : b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "Target: b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_QM_0JUnkKLy"
      },
      "source": [
        "BATCH_SIZE = 64\r\n",
        "\r\n",
        "BUFFER_SIZE = 10000\r\n"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zykuoutykr9G"
      },
      "source": [
        "dataset = (dataset\r\n",
        "           .shuffle(BUFFER_SIZE)\r\n",
        "           .batch(BATCH_SIZE,drop_remainder=True)\r\n",
        "           .prefetch(tf.data.experimental.AUTOTUNE)\r\n",
        "           )"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4juYtjWSlDrO",
        "outputId": "0fb46c61-14bf-413c-e170-9ebcba7d3152"
      },
      "source": [
        "dataset"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PrefetchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwoZ3h61lEqJ"
      },
      "source": [
        "vocab_size = len(vocab)\r\n",
        "embedding_dim = 256\r\n",
        "rnn_units = 1024"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPro-4SjlsI3"
      },
      "source": [
        "class MyModel(tf.keras.Model):\r\n",
        "  \r\n",
        "  \r\n",
        "  def __init__(self,vocab_size,embedding_dim,rnn_units):\r\n",
        "    super().__init__(self)\r\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size,embedding_dim)\r\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units,\r\n",
        "                                   return_sequences=True,\r\n",
        "                                   return_state=True)\r\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\r\n",
        "  \r\n",
        "\r\n",
        "\r\n",
        "  def call(self,inputs,states=None,return_state=False,training=False):\r\n",
        "    x = inputs\r\n",
        "    x = self.embedding(x,training=training)\r\n",
        "    if states is None:\r\n",
        "      states = self.gru.get_initial_state(x)\r\n",
        "    x,states = self.gru(x, initial_state=states,training=training)\r\n",
        "    x = self.dense(x,training=training)\r\n",
        "    if return_state:\r\n",
        "      return x,states\r\n",
        "    else:\r\n",
        "      return x"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYVORhz8msTx"
      },
      "source": [
        "model = MyModel(vocab_size= len(ids_from_chars.get_vocabulary()),\r\n",
        "                embedding_dim=embedding_dim,\r\n",
        "                rnn_units = rnn_units\r\n",
        "    \r\n",
        ")"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_qcwTzFZoqf3",
        "outputId": "bab3d7d5-5402-45ee-eb75-1af4e055be22"
      },
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\r\n",
        "  example_batch_predictions  = model(input_example_batch)\r\n",
        "  print(example_batch_predictions.shape)"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 100, 67)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPV160Z0pnDq"
      },
      "source": [
        "* **64** is my batch_size\r\n",
        "* **100** is my sequence_length\r\n",
        "* **67** is my vocab_size"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T8DAJ6MBpKv0",
        "outputId": "dd6506f8-4eac-41ef-e965-e1b238607d90"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"my_model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      multiple                  17152     \n",
            "_________________________________________________________________\n",
            "gru_1 (GRU)                  multiple                  3938304   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              multiple                  68675     \n",
            "=================================================================\n",
            "Total params: 4,024,131\n",
            "Trainable params: 4,024,131\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFR7Bxgcp3nk"
      },
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\r\n",
        "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p-7J7kAdqBMI",
        "outputId": "84d7ae21-b7ba-4338-9c07-c63794f4c992"
      },
      "source": [
        "sampled_indices"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([61, 34, 53,  7, 42, 54, 14, 12,  8, 58, 45, 27, 23, 62, 52, 23, 26,\n",
              "       13, 38, 63, 64, 54, 27, 54, 18, 14, 60, 65, 61, 32, 29, 52, 19, 19,\n",
              "       27, 41, 39, 58, 36, 63,  2, 18,  5, 43, 59, 11, 44, 38, 65, 55,  8,\n",
              "       62, 33, 44, 64, 13, 39, 55, 45, 35, 38, 61, 39, 53, 51, 10, 62, 53,\n",
              "       42, 63, 10, 44, 41, 45, 23, 20, 23, 45, 65, 21, 11, 20, 38, 65, 66,\n",
              "       30,  9,  8, 54, 39,  3, 11, 66, 36,  0,  8,  1, 45, 32, 66])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8sgreZLMqFiz",
        "outputId": "832247b6-9db8-4430-bd94-a4b0d2a4b88b"
      },
      "source": [
        "print(\"Input \\n\",text_from_ids(input_example_batch[0].numpy()))"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input \n",
            " tf.Tensor(b\"t wash'd\\nMy nose that bled, or foil'd some debile wretch.--\\nWhich, without note, here's many else ha\", shape=(), dtype=string)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dCR8lDYpqO0S",
        "outputId": "f44889a5-fedf-46c4-e4fe-bac6d01f9da4"
      },
      "source": [
        "print(\"Next Char Predication:\\n\",text_from_ids(sampled_indices).numpy())"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Next Char Predication:\n",
            " b\"uTm'bn?:,reMIvlIL;XwxnMnD?tyuROlEEMaYrVw\\nD$cs3dXyo,vSdx;YoeUXuYmk.vmbw.daeIFIeyG3FXyzP-,nY 3zV,[UNK]eRz\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ri4GlFt5qeOb"
      },
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lq-Eso6-qw2c"
      },
      "source": [
        "example_batch_loss  = loss(target_example_batch,example_batch_predictions)\r\n",
        "mean_loss = example_batch_loss.numpy().mean()"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "US1Y-0Nfq-U1",
        "outputId": "2a29a57c-9a38-4ec8-b304-6c223f870bb5"
      },
      "source": [
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\r\n",
        "print(\"Mean loss:        \", mean_loss)"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction shape:  (64, 100, 67)  # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:         4.2058086\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nmxbkeLWq_0W",
        "outputId": "bb940f1b-d87b-43f3-98d6-6428842e4417"
      },
      "source": [
        "tf.exp(mean_loss).numpy()"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "67.074814"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6k68G7FMrF_w"
      },
      "source": [
        "model.compile(optimizer='adam',loss=loss)"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OYsH4bwYrPpv"
      },
      "source": [
        "checkpoint_dir = './training_checkpoints'\r\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir,\"ckpt_{epoch}\")\r\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\r\n",
        "    filepath=checkpoint_prefix,\r\n",
        "    save_weights_only=True)"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-eR16f-roRB"
      },
      "source": [
        "EPOCHS =2"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DMj8hqnVrsRC",
        "outputId": "360e4507-78f8-4d3a-897e-525ec5e6ef07"
      },
      "source": [
        "history = model.fit(dataset,epochs=EPOCHS,\r\n",
        "                    callbacks=[checkpoint_callback])"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "172/172 [==============================] - 974s 6s/step - loss: 3.3429\n",
            "Epoch 2/2\n",
            "172/172 [==============================] - 991s 6s/step - loss: 2.0952\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0f94coEtSsd"
      },
      "source": [
        "### Generate text\r\n",
        "![](https://www.tensorflow.org/tutorials/text/images/text_generation_sampling.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efZXqwiaoWh-"
      },
      "source": [
        "class OneStep(tf.keras.Model):\r\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\r\n",
        "    super().__init__()\r\n",
        "    self.temperature=temperature\r\n",
        "    self.model = model\r\n",
        "    self.chars_from_ids = chars_from_ids\r\n",
        "    self.ids_from_chars = ids_from_chars\r\n",
        "\r\n",
        "    # Create a mask to prevent \"\" or \"[UNK]\" from being generated.\r\n",
        "    skip_ids = self.ids_from_chars(['','[UNK]'])[:, None]\r\n",
        "    sparse_mask = tf.SparseTensor(\r\n",
        "        # Put a -inf at each bad index.\r\n",
        "        values=[-float('inf')]*len(skip_ids),\r\n",
        "        indices = skip_ids,\r\n",
        "        # Match the shape to the vocabulary\r\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())]) \r\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\r\n",
        "\r\n",
        "  @tf.function\r\n",
        "  def generate_one_step(self, inputs, states=None):\r\n",
        "    # Convert strings to token IDs.\r\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\r\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\r\n",
        "\r\n",
        "    # Run the model.\r\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits] \r\n",
        "    predicted_logits, states =  self.model(inputs=input_ids, states=states, \r\n",
        "                                          return_state=True)\r\n",
        "    # Only use the last prediction.\r\n",
        "    predicted_logits = predicted_logits[:, -1, :]\r\n",
        "    predicted_logits = predicted_logits/self.temperature\r\n",
        "    # Apply the prediction mask: prevent \"\" or \"[UNK]\" from being generated.\r\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\r\n",
        "\r\n",
        "    # Sample the output logits to generate token IDs.\r\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\r\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\r\n",
        "\r\n",
        "    # Convert from token ids to characters\r\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\r\n",
        "\r\n",
        "    # Return the characters and model state.\r\n",
        "    return predicted_chars, states"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHX1lSufo-bO"
      },
      "source": [
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PgIQzCySrlgF",
        "outputId": "1ea5ae3b-dbd6-482e-e984-6446b3637a61"
      },
      "source": [
        "start = time.time()\r\n",
        "states = None\r\n",
        "next_char = tf.constant(['ROMEO:'])\r\n",
        "result = [next_char]\r\n",
        "\r\n",
        "for n in range(1000):\r\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\r\n",
        "  result.append(next_char)\r\n",
        "\r\n",
        "result = tf.strings.join(result)\r\n",
        "end = time.time()\r\n",
        "\r\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\r\n",
        "\r\n",
        "print(f\"\\nRun time: {end - start}\")"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ROMEO:\n",
            "Haver, and herred, then hels a friss this hamp hepperourep;\n",
            "Aroon make God my freen ag that ple wauld Lifter?\n",
            "\n",
            "Gord:\n",
            "Ay, to thes I Grom the not then I\n",
            "To I kand bet theme\n",
            "Fouthio, faild alluf, the lealy.\n",
            "\n",
            "MORDANEE:\n",
            "Foo, by tonquid med, Ay sweat Nor?\n",
            "\n",
            "KENINSIT:\n",
            "My to the kind.\n",
            "\n",
            "merd Ad one way thouss know, in os I have say be off to whe inceres and that?\n",
            "UMes you savee haw\n",
            "Kivese enflate or our thesf things Yor wos longe anturd\n",
            "I\n",
            "seall and my wainger med ree foo frem fir my kning;\n",
            "my 'tis and once come ol cheed,\n",
            "Toll the satatrence aven you.\n",
            "\n",
            "Secon.\n",
            "The, where allot-gonder-Chatt hurd make.\n",
            "\n",
            "CAMIO:\n",
            "Fare theor have soon fear he is this unis;\n",
            "Ay preate, preat chomour have comeming.\n",
            "\n",
            "VARYY:\n",
            "If ivond Hist kime to thy seeph the king\n",
            "Yot hear ublid-be outange corded\n",
            "As thou'd so vist it him the raves: my los:\n",
            "Wicluch ravore in wry ferter.\n",
            "\n",
            "First My Clitenty of I\n",
            "Am all gild bum flow I I love,\n",
            "I to make is I smecken it a plock:\n",
            "The causeres a diny the this fanty you nom.\n",
            "\n",
            "First:\n",
            "Wh trambst Gle \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 2.532912254333496\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9qeKlZ6csZiL",
        "outputId": "8e1c1baf-114b-43f4-857f-5ab5df76e56d"
      },
      "source": [
        "start = time.time()\r\n",
        "states = None\r\n",
        "next_char = tf.constant(['ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:'])\r\n",
        "result = [next_char]\r\n",
        "\r\n",
        "for n in range(1000):\r\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\r\n",
        "  result.append(next_char)\r\n",
        "\r\n",
        "result = tf.strings.join(result)\r\n",
        "end = time.time()\r\n",
        "\r\n",
        "print(result, '\\n\\n' + '_'*80)\r\n",
        "\r\n",
        "\r\n",
        "print(f\"\\nRun time: {end - start}\")"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[b\"ROMEO:\\nMaight in this non?\\n\\nThes Lirhim,\\ntheer payse Masceme of my lets.\\n's not icloved thou abin\\nWesoush him blows, fere in hig dishag,\\nThengefoul seast that ie the un preatus that in you,\\nThee! frise these out tiret, lots camo.\\n\\nLADY CIONCEMHA:\\nWhow, ceford's ale!\\n\\nAULOLENE: shey dad: it? What kenglenfarrass me; by ty ane\\nTo eny sence mo juete my lende\\nTo trow pasty hear dadse try qoue'e wail;\\nThrwe! say, 'tin I lo,\\nFor sen all agh thater:\\nThe kinds of the slole af the eremous beo, of thoudd,\\nHitht by thou papfot?\\n\\nJlAULENS:\\nMy thou agan themel if is anch'd will of Lo, cho packs,\\nAnd ame to mask not\\nThat a patrery to mor, in shemely of thak tur\\nin granken dy ustersed's mastor thim\\nFrempece,\\nMowhank, I may be come son, ho my, bone Beart a farte.\\n\\nCARUME:\\nIt in anvithouse\\nbul nourd wifl Vaistir!\\n\\nPirilt Cameling\\napore sluen's loath ar' in erfor;\\nAnd that! ar censte was she to my, lose you sleign?\\n\\nANCELYO:\\n\\nHers:\\nFad lisiosss stall hasth you mant doge,\\nTheake'd a flish chore it ridiont subry\"\n",
            " b\"ROMEO:\\nSo coon thise, giving ammit, in thesel is thate\\nbo the lifeme hos rewath'd not betien;\\nWo do depor, noctrrie porm.\\nPatesuro\\nFremsent the crunghe wowncay denel, as arrowenay:\\nThough the plonsient not prald till apiscion Mreang; kill tree\\nThos fortuch ripedids all that's is I have omec\\nThat but tho esting Is life now bruakel me not,\\nFrom moren masu not menan; my seence mak with a hore.\\n\\nVININAS:\\nWho a, lucks, it dase a theiro?\\n\\nPOLPOUSIO:\\nWe comm herreft hanth ufonge, I omal'd my with mare\\nBur this fif dord hered upos if twe breag.\\nBue my trut exer! they mais dake, you, farmy\\nHech carrsuars, a tyen as hard bee feilp\\nKish ifmere: if this anf of the,\\nThou anfod prechin! my lake.\\n\\nBown:\\nSty, mathin.\\n\\nCLOLANE SIHAND:\\nWhis bus canle brience lef thuse with new,\\ngrowemblouso it to te dould not polsedsing butkels scavedss,\\nThere ree hom he; por forour farst to dot\\nAnk frur dosees reedongare.\\n\\nPondY:\\nThe bead my laid, hor have of chating? -wers Wealing!\\nThe lordus all sage yow: stried pail timpl\"\n",
            " b\"ROMEO:\\nTiell me nobr to me?\\n\\nVERCINIUS:\\nMy tho fall'd,\\nSay, aray'd thit'm hither\\nSo, bake a make thy bith to the musperes'd, I shaple follms;\\nThy groult, whil free ithenes, af is the offod\\nAng shood yeal deander scome the to dremper to back and sie?\\nTho dutien! sich of shee, spey I my are amford\\nAnd thee brimenour bay ferrout retald\\nbo hasped her ley bed you truaked those.\\n\\nASIO:\\nGlam shepe, my bood thee?\\n\\nBRANVET'N\\nLINCENTER:\\nMy than astes Tift them evet Whoughts coth righ asperreg\\nAnd thin my conothor't.\\n\\nPRADF:\\nThis iinght thick'd us all evour'd well be me.\\n\\nBandPULLIO:\\nIn toalg yie!\\nStiy sandala,'l spenow mail shill lotd ho maquer:\\nAnd server save the hear not sumfit I day I dake no.\\n\\nSlicest mastan,\\nMare no dather all king of I prikn come tor sherce be de if it,\\nPreeven, as a the youncits to an tuee be amow it.\\n\\nCLAPYOUS:\\nSee of itn'st the slayn,\\nAnd have the kend'd doulbon, no mare aty say\\nHo now the is a streblions antiont thasel Cxifit.\\n\\nFirst Sextiless; sir, at singedt.\\n\\nFORCEY:\\nMy \"\n",
            " b\"ROMEO:\\nMay, likn that kive it man\\nWas 'twir'd thoug seadf, thyure oft my love, O bely for.\\nThat mase dayes apalt theseffor is envert theek be that.\\n\\nLUCOSTER:\\nBo dood, shall mincles ang on thy sictraist---\\nTike the years? The, is not frommance you brost he,\\nTo king to betence that wher theses to light have, many.\\n\\nCLAUDELLO:\\nHa'gh mak est ow thouss lades, and gay.\\n\\nWisced\\nChead dight, mukl that earss speef comele,\\nAnd she trem-'Tabliok calst be not me he arow, ser fous he\\nThan I dere fee her thay\\nYou peere subre dess to dod theme secable so cooth. So retter!\\n\\nRamoul's:\\nWhat wingott him suce us agous by his doad the\\nDukm; Shall the subte have to clifeble; hell fromonath not suckens,\\nAnd radagt for the rastly soe my toos qoun nop end\\ndan knapt of a conell of his for his dorious agniever.\\n\\nFirst,\\nSpangly stake of, here for efor deer wer feesexand,\\nThou have the enture af woustersse my anl the\\npragw befor' of not wotenome to he do, the kend,\\nThe ringe, shonght you, aod las, have wan as gessist.\\n\"\n",
            " b\"ROMEO:\\nI pase rech me well; and weecly as withore thes eree\\nThinker't shill walf them, of Glyiend to ne have she\\nBur shy pice be deire ile.\\n\\nMINGBREONNERL:\\nHese soult-nem, wall we lef his be fee douth forded thin oue\\nThey the parce mathin.\\n\\nSouther:\\nWith dutcerby tild. Hor mus cave, bofur my higlows, bun my lorg,\\n-her belain, my hieds, aph ligkl, goed to his perim.\\nFoult she god the anering he eress!\\nWhisk the dast swates mise mo sarmorm, tued wike loves there herd be\\nspopptel to efime, where atale?\\n\\nTROM:\\nTrow thou hem:\\nAnd thes wen there;\\nSom. For shalk He carse she with im theme iffounts\\nThere nom, mewilk us thy then\\n\\nSINCANHES:\\nI totave, whor now say, and chark I'll and as and to she libe.\\n\\nSinot Aod not thould do there\\nWhich the forhterty to withy manterance of strapy of thiskna\\nThuseor sintle; farsing have bre digle, Foull's and tloueds he:\\nAnd nen the fell.\\nYou ar whou blan an all; my lirguly, afte is a lige\\nMrusce armither of thise pay hut cave thee inseather'd trings yee.\\n\\nFLARIY:\\nB\"], shape=(5,), dtype=string) \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 4.625087022781372\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TtfxbjlNs1tM",
        "outputId": "5c7220e5-7c71-47ea-bb75-8a525fcbd56a"
      },
      "source": [
        "tf.saved_model.save(one_step_model,'one_step')\r\n"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x7f8183db96a0>, because it is not built.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as gru_cell_1_layer_call_fn, gru_cell_1_layer_call_and_return_conditional_losses, gru_cell_1_layer_call_fn, gru_cell_1_layer_call_and_return_conditional_losses, gru_cell_1_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as gru_cell_1_layer_call_fn, gru_cell_1_layer_call_and_return_conditional_losses, gru_cell_1_layer_call_fn, gru_cell_1_layer_call_and_return_conditional_losses, gru_cell_1_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: one_step/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: one_step/assets\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9nvn0bjvtBU1"
      },
      "source": [
        "one_step_reloaded = tf.saved_model.load('one_step')"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hw072NtntNbK",
        "outputId": "9bc16d2c-883f-4244-c950-46f196c8ab54"
      },
      "source": [
        "states = None\r\n",
        "next_char = tf.constant(['ROMEO:'])\r\n",
        "result = [next_char]\r\n",
        "\r\n",
        "for n in range(100):\r\n",
        "  next_char, states = one_step_reloaded.generate_one_step(next_char, states=states)\r\n",
        "  result.append(next_char)\r\n",
        "print(tf.strings.join(result)[0].numpy().decode(\"utf-8\"))"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function recreate_function.<locals>.restored_function_body at 0x7f817b30f598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function recreate_function.<locals>.restored_function_body at 0x7f817b30f598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:6 out of the last 6 calls to <function recreate_function.<locals>.restored_function_body at 0x7f817b30f598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:6 out of the last 6 calls to <function recreate_function.<locals>.restored_function_body at 0x7f817b30f598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "ROMEO:\n",
            "Mastingt uthe to miperort?\n",
            "\n",
            "JUCILIF:\n",
            "Clids then my frat os a mard.\n",
            "Go sorblecck? Which sir's fallic\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qIQd9dQgtk7i"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}